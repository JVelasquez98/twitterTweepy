{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import datetime\r\n",
    "import itertools\r\n",
    "\r\n",
    "from twarc.client2 import Twarc2\r\n",
    "from twarc.expansions import ensure_flattened\r\n",
    "\r\n",
    "from twarc_csv import CSVConverter\r\n",
    "import json"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "rate limit exceeded: sleeping 168.82536029815674 secs\n",
      "rate limit exceeded: sleeping 909.7469599246979 secs\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# Your bearer token here\r\n",
    "t = Twarc2(bearer_token=\"beareretoken\")\r\n",
    "\r\n",
    "# Start and end times must be in UTC\r\n",
    "start_time = datetime.datetime(2021, 3, 10, 0, 0, 0, 0, datetime.timezone.utc)\r\n",
    "end_time = datetime.datetime(2021, 3, 22, 0, 0, 0, 0, datetime.timezone.utc)\r\n",
    "\r\n",
    "# search_results is a generator, max_results is max tweets per page, 500 max for full archive search.\r\n",
    "search_results = t.search_all(query=\"place_country:CG\", start_time=start_time, end_time=end_time, max_results=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get all results page by page:\r\n",
    "\r\n",
    "#type(search_results)\r\n",
    "\r\n",
    "for page in search_results:\r\n",
    "    # Do something with the whole page of results:\r\n",
    "    #print(page)\r\n",
    "    # or alternatively, \"flatten\" results returning 1 tweet at a time, with expansions inline:\r\n",
    "\r\n",
    "    for tweet in ensure_flattened(page):\r\n",
    "        #Do something with the tweet\r\n",
    "        with open(\"json_output.json\", \"w+\") as f:\r\n",
    "            f.write(json.dumps(tweet) + \"\\n\")\r\n",
    "       #print(tweet)\r\n",
    "    # Stop iteration prematurely, to only get 1 page of results.\r\n",
    "\r\n",
    "    #break\r\n",
    "\r\n",
    "print(\"Finished crawling, saving csv.\")\r\n",
    "with open(\"json_output.json\", \"r\") as infile:\r\n",
    "    with open(\"output.csv\", \"w\") as outfile:\r\n",
    "        converter = CSVConverter(infile=infile, outfile=outfile)\r\n",
    "        converter.process()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82c0337de1a4f0e6ffb948884f01fcd5972a5bb881ecd94c1c3edf43db885021"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}